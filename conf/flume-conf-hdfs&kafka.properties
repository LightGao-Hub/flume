# 组装agent
a1.sources = src_1
#这里配置了两个channel，因为source读取的event可以给多个channel
a1.channels = ch_1 ch_2
#也配置了两个Sink，因为一个Sink只能对应一个channel
a1.sinks = sink_1 sink_2

# 配置 source：从目录中读取文件，不能读取正在写入的文件
a1.sources.src_1.type = spooldir
# 此source读取的event给了两个channel
a1.sources.src_1.channels = ch_1 ch_2
a1.sources.src_1.spoolDir = /home/hadoop/flume/log/
#以下两行配置相辅相成，可以查看官网，意思是只有同时满足以下两个正则表达式才会忽略此文件文件
# 我们想让flume忽略正在写入的文件，也就是app.log文件，所以ignorePattern=^.*log$的意思是 忽略log结尾的文件！
# 而我们生成的文件是以时间戳为结尾，所以不会忽略！
a1.sources.src_1.includePattern=^.*$
a1.sources.src_1.ignorePattern=^.*log$
#何时删除已完成的文件：从不或立即
a1.sources.src_1.deletePolicy= never
#是否添加存储绝对路径文件名的头。
a1.sources.src_1.fileHeader = true
## 设置时间戳拦截器
a1.sources.src_1.interceptors = i1
a1.sources.src_1.interceptors.i1.type = timestamp

# 配置 channel：缓存到文件中
a1.channels.ch_1.type = memory
# channels最大存储的event数，默认100 这里配置的是10000
a1.channels.ch_1.capacity = 10000
#一次事务中，写入和读取的event最大数，要和上面一致：10000
a1.channels.ch_1.transactionCapacity = 10000


# 配置 sink：保存到hdfs中
a1.sinks.sink_1.channel = ch_1
a1.sinks.sink_1.type = hdfs
#HDFS的存储路径,按照flume拦截器的时间戳来创建文件
a1.sinks.sink_1.hdfs.path = hdfs://master:9000/flume/log/%Y-%m-%d
# 文件的前缀是logs
a1.sinks.sink_1.hdfs.filePrefix = logs
#下面三个roll是三个选择，意思是HDFS文件滚动生成的三种选择，可以是时间也可以是大小和数量，这里我们设置的时间10秒
a1.sinks.sink_1.hdfs.rollInterval = 10
a1.sinks.sink_1.hdfs.rollSize = 0
a1.sinks.sink_1.hdfs.rollCount = 0
#flume批量写入HDFS的Event数量，默认100
a1.sinks.sink_1.hdfs.batchSize = 100
# hadoop自带的Sequence文件格式，可以是Text或者Writable(序列化)， 默认时Writable格式
a1.sinks.sink_1.hdfs.writeFormat = Text
# 文件的最小副本数，默认是3
a1.sinks.sink_1.hdfs.minBlockReplicas = 1

# 配置ch_2
a1.channels.ch_2.type = memory
a1.channels.ch_2.capacity = 10000
a1.channels.ch_2.transactionCapacity = 10000

# 下面是存储到kafka的配置！ 写法参照了flume官方给出的例子
a1.sinks.sink_2.channel = ch_2
a1.sinks.sink_2.type = org.apache.flume.sink.kafka.KafkaSink
#选择kafka的主题，主题要提前创建！这里创建的主题是mytopic
a1.sinks.sink_2.kafka.topic = mytopic
# kafka的broker列表，可以配置一个，通过一个即可找到所有broker，当然为了保险起见建议配置多个，一个宕机可以找另一个
a1.sinks.sink_2.kafka.bootstrap.servers = master:9092
# 表示kafka生产者端单词批量发送的消息条数，该值根据环境调整，越多性能越好，但是也会有风险，默认值是100，这里设置20，表示累计20条，kafka生产者一次性发送20条
a1.sinks.sink_2.kafka.flumeBatchSize = 20
# 这里的ack书中有介绍，设置为1 表示broker成功写入后，返回成功消息继续写入
a1.sinks.sink_2.kafka.producer.acks = 1
# 延迟发送消息时间，起到一个降低负载的作用，设置值只要大于0，则表示不会立即发送消息，只有当消息达到flumeBatchSize设置的值的时候才会发送出去！
a1.sinks.sink_2.kafka.producer.linger.ms = 1
# 压缩格式，起到优化作用，将发送消息选择压缩格式，有多种格式，可以参照书中所写！
a1.sinks.sink_2.kafka.producer.compression.type = snappy

# 关于kafka的收集首先要安装kafka，集群或者单机都可以，这里我们设置的是单机的kafka，启动kafka调用的配置文件时config自带的默认配置：server.properties
# kafka将收集的日志数据放置在了/tmp目录下，消费者会去/tmp目录下读取消费，这里可以看咕泡kafka的视频
# 接下来先将此文件放到flume的conf目录下，注意！！！要将此文件改名，因为&符号linux会认为是两个文件，就会发生找不到文件异常，我这里将此文件更改为：flume-config.properties
# 然后启动kafka创建kafka的主题topic：mytopic 动日志生成器生成日志
# 启动zookeeper命令：zookeeper-server-start.sh config/zookeeper.properties  先启动zookeeper
# 再启动kafka: kafka-server-start.sh config/server.properties
# 再创建主题：kafka-topics.sh  --create  --zookeeper  localhost:2181  --replication-factor  1  --partitions  1  --topic  mytopic
# 查看主题：kafka-topics.sh  --list  --zookeeper  localhost:2181
# 然后启动日志生成器，生成日志后，启动flume：flume-ng agent -n a1 -c /home/hadoop/usr/flume/conf -f /home/hadoop/usr/flume/conf/flume-config.properties
# 控制台查看kafka消费者命令：kafka-console-consumer.sh  --zookeeper  localhost:2181 --topic  mytopic  --from-beginning
# 此时每隔一段时间此窗口就会打印传输的数据，至此完成了flume+kafka+hdfs的测试，如果想让sparkStreaming连接hdfs和kafka也非常简单，参照林子雨项目即可！
