# 组装agent
a1.sources = src_1
a1.channels = ch_1
a1.sinks = sink_1

# 配置 source：从目录中读取文件，不能读取正在写入的文件
a1.sources.src_1.type = spooldir
# 此source读取的event给了两个channel
a1.sources.src_1.channels = ch_1
a1.sources.src_1.spoolDir = /home/hadoop/flume/log/
#以下两行配置相辅相成，可以查看官网，意思是只有同时满足以下两个正则表达式才会忽略此文件文件
# 我们想让flume忽略正在写入的文件，也就是app.log文件，所以ignorePattern=^.*log$的意思是 忽略log结尾的文件！
# 而我们生成的文件是以时间戳为结尾，所以不会忽略！
a1.sources.src_1.includePattern=^.*$
a1.sources.src_1.ignorePattern=^.*log$
#何时删除已完成的文件：从不或立即
a1.sources.src_1.deletePolicy= never
#是否添加存储绝对路径文件名的头。
a1.sources.src_1.fileHeader = true
## 设置时间戳拦截器
a1.sources.src_1.interceptors = i1
a1.sources.src_1.interceptors.i1.type = timestamp

# 配置 channel：缓存到文件中
a1.channels.ch_1.type = memory
# channels最大存储的event数，默认100 这里配置的是10000
a1.channels.ch_1.capacity = 10000
#一次事务中，写入和读取的event最大数，要和上面一致：10000
a1.channels.ch_1.transactionCapacity = 10000


# 配置 sink：保存到hdfs中
a1.sinks.sink_1.channel = ch_1
a1.sinks.sink_1.type = hdfs
#HDFS的存储路径,支持按照时间来分区，看书即可，这里我们按照flume拦截器的时间戳来创建文件名，这个目录别忘了创建！
a1.sinks.sink_1.hdfs.path = hdfs://master:9000/flume/log/%Y-%m-%d
# 文件的前缀是logs
a1.sinks.sink_1.hdfs.filePrefix = logs
#下面三个roll是三个选择，意思是HDFS文件滚动生成的三种选择，可以是时间也可以是大小和数量，这里我们设置的时间10秒
a1.sinks.sink_1.hdfs.rollInterval = 10
a1.sinks.sink_1.hdfs.rollSize = 0
a1.sinks.sink_1.hdfs.rollCount = 0
#flume批量写入HDFS的Event数量，默认100
a1.sinks.sink_1.hdfs.batchSize = 100
# hadoop自带的Sequence文件格式，可以是Text或者Writable(序列化)， 默认时Writable格式
a1.sinks.sink_1.hdfs.writeFormat = Text
# 文件的最小副本数，默认是3
a1.sinks.sink_1.hdfs.minBlockReplicas = 1

# 此文件配置好后，将此文件放入到flume 的conf目录下，启动log日志生成器，创建hdfs的log目录后启动flume-ng 命令如下：
# flume-ng agent -n a1 -c /home/hadoop/usr/flume/conf -f /home/hadoop/usr/flume/conf/flume-conf-hdfs.properties
# 此时flume即将我们生成log日志抽取到hdfs中，如果你想看flume自身的log日志信息，因为我们没有指定输出的log目录，所以默认在
# conf目录下会有一个log目录，里面就是flume.log文件
# 那么执行完flume命令后，我们再去生成log日志的地方：/home/hadoop/flume/log/  你会发现文件都以completed结尾，这代表都文件已被完整读取！
# 注意：并不是有多少日志对应多少hdfs的文件数量，因为中间 有一个channel缓冲，所以时间上会有延迟！